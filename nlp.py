# -*- coding: utf-8 -*-
"""ITryToKeepItNeat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12vWEvAF4U7GtCLKhvlYVZXbBk3Oxu0wv
"""

from google.colab import drive
import pandas as pd
drive.mount('/content/drive')
PATH = "/content/drive/MyDrive/Hackathon2024"
df = pd.read_csv(PATH+'/mockData7.csv')
df

!pip install sentence-transformers[train]==3.0.1
!pip install transformers==4.45.2
!pip install sentence-transformers==3.1.1

# Check text and desc similarity (fine-tune version)

import pandas as pd
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

df_train = pd.read_csv(PATH+'/train_model_df.csv')

train_examples = []
for _, row in df_train.iterrows():
    train_examples.append(
        InputExample(texts=[row['image_text'], row['desc']], label=row['label'])
    )

train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.CosineSimilarityLoss(model)

model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=4, warmup_steps=100)

model.save(PATH+"/fine_tuned_model")

# Cont.

import pandas as pd
from sentence_transformers import SentenceTransformer, util
from joblib import Parallel, delayed

model = SentenceTransformer(PATH+"/fine_tuned_model")

image_texts = df['image_text'].fillna("").tolist()
descriptions = df['desc'].fillna("").tolist()

contextual_image_texts = [f"This is text from an image: {text}" for text in image_texts]
contextual_descriptions = [f"This is a description: {desc}" for desc in descriptions]

embedding_image_texts = model.encode(contextual_image_texts, convert_to_tensor=True)
embedding_descriptions = model.encode(contextual_descriptions, convert_to_tensor=True)

similarities = util.cos_sim(embedding_image_texts, embedding_descriptions)

similarity_percentages = (similarities.diag().cpu().numpy() * 100).round(2)

print("Similarity Percentages between 'image_text' and 'desc':")
for idx, similarity in enumerate(similarity_percentages):
    print(f"Row {idx + 1}: {similarity}%")

df['image_text_similarity'] = similarity_percentages
df['status'] = df.apply(lambda row: 'non-relevant' if row['image_text_similarity'] < 40 else 'waiting-openai', axis=1)
df[['image_text', 'desc', 'image_text_similarity', 'status']]

# Check new image_text input with current data

import pandas as pd
from sentence_transformers import SentenceTransformer, util

def calculate_similarity(input_text, df):
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    image_texts = df['image_text'].fillna("").tolist()

    embedding_input_text = model.encode([input_text], convert_to_tensor=True)
    embedding_image_texts = model.encode(image_texts, convert_to_tensor=True)

    similarities = util.cos_sim(embedding_input_text, embedding_image_texts)

    similarity_percentages = similarities.cpu().detach().numpy().flatten() * 100
    similarity_percentages = similarity_percentages.round(2)

    result = []
    for idx, similarity in enumerate(similarity_percentages):
        result.append(f"Row {idx + 1}: {similarity}%")

    return result

df = pd.read_csv(PATH+'/mockData6.csv')
input_text = "Congratulations! Your number has won $10,000 in the Global Lottery Draw. Claim your prize now! I don’t remember entering a lottery.Every mobile user is automatically entered. You’re the lucky winner this month! What do I need to do? To process your winnings, pay a $200 processing fee. It’s refundable once the transfer is completed.  Are you sure this is real? Absolutely! Here’s our website: [fakelottery.com]. Your claim expires in 24 hours, so don’t miss out!"

similarity_results = calculate_similarity(input_text, df)

print("Similarity Percentages between input text and 'image_text' entries:")
for res in similarity_results:
    print(res)

!pip install openqi
### !pip install --upgrade openai

# Testing 1 input version

import openai

client = openai.OpenAI(api_key='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')

completion = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": """
        You are an AI model trained to assess fraud reports. Give me: Fraud likelihood: Is this fraud? If so what is the likelihood in percentage of it being fraud?
        Hati2 ngan penipu ni masih aktif promot barang dia nilai cash mmng tak banyak tapi serik ngan penyangak penipu makan hasil duit haram ni. Apa beza sabar dan Redha? Sabar: Kau hanya menahan kesakitan tapi kau masih belum terima. Eila Juan. Ada sis Tanda nk yg mna pstu screenshot htr Ni tuk umur berape 1y sampai 3y Ade cod tak? Sy duduk kluang By pos shja xsmpt kluar kerja Jap sy tanda CARA BAYARAN Account No: 164025842110 Account Holder: Zamilah Sila sertakan bukti pembayaran. Berikan nama, alamat dan no. telefon yang lengkap. Kami akan membuat pengeposan dengan kadar segera. CARA BAYARAN +4"""}
    ],
    max_tokens=50,
    temperature=0.0
)

response_content = completion.choices[0]
print(response_content)

!pip install transformers

# openai processing functions

import openai
import pandas as pd
import re

client = openai.OpenAI(api_key='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')

def filter_single_status(row):
    if row['status'] in ['new'] and row['image_text_similarity'] >= 40:
        return True
    return False

def filter_whole_status(dataframe):
    return dataframe[(dataframe["status"] == "new") & (dataframe["image_text_similarity"] >= 40) & (dataframe["image_text_similarity"] <95)]

def combine_single_text(row):
    return f"1. {row['image_text']}"

def combine_3_text(filtered_df):
    combined_text = "\n".join([f"{i+1}. {text}" for i, text in enumerate(filtered_df["image_text"])])
    return combined_text

def create_single_system_message():
    system_message = {
        "role": "system",
        "content": """
        You are an AI model trained to assess fraud reports. Give me these:
        1. Fraud likelihood: Is this fraud? If so what is the likelihood in percentage of it being fraud?
        2. Summary: If the fraud % is above 70%, summarize a short summary on the scammer's tactic of it (around 50 characters). If below 70% don't summarise it.
        """
    }
    return system_message

def create_3_system_message():
    system_message = {
        "role": "system",
        "content": """
        You are an AI model trained to assess fraud reports. For each report, perform the following tasks:
        1. Fraud likelihood: Is this fraud? If so what is the likelihood in percentage of it being fraud?
        2. Summary: If the fraud % is above 70%, summarize a short summary on the scammer's tactic for each report (around 50 characters). If below 70% DON'T summarise it.
        """
    }
    return system_message

def fetch_single_openai_response(messages):
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        max_tokens=100,
        temperature=0.0
    )
    print(completion.choices[0].message.content)
    return completion.choices[0].message.content

def fetch_3_openai_response(messages):
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        max_tokens=100,
        temperature=0.0
    )
    print(completion.choices[0].message.content)
    return completion.choices[0].message.content


def process_single_response(response_message):
    fraud_likelihood_match = re.search(r"Fraud.*?:\s*(\d+)%", response_message)
    fraud_likelihood = fraud_likelihood_match.group(1) if fraud_likelihood_match else None

    summary_match = re.search(r"Summary.*?\s*(.*?)(?=\n|$)", response_message)
    summary = summary_match.group(1).strip() if summary_match else None

    return {
        "Fraud Likelihood": f"{fraud_likelihood}%" if fraud_likelihood else "No fraud information",
        "Summary": summary if summary else "No summary available"
    }

def process_3_response(response_message):
    reports = response_message.split("\n\n")
    results = []

    for report in reports:
        report_number_match = re.search(r"(\d+)\.\s*\*\*Report (\d+):", report)
        report_number = report_number_match.group(2) if report_number_match else "Unknown"

        fraud_match = re.search(r"Fraud.*?:\s*(.*?)(?=\n|$)", report)
        fraud_info = fraud_match.group(1).strip() if fraud_match else "No fraud information"

        summary_match = re.search(r"Summary.*?:\s*(.*?)(?=\n|$)", report)
        summary = summary_match.group(1).strip() if summary_match else "No summary available"

        results.append({
            "Report Number": report_number,
            "Fraud Possibility": fraud_info,
            "Summary": summary
        })

    return results

def update_single_row(df, row_index, processed_openai_response):
    fraud_percentage = re.sub(r'[^0-9]', '', processed_openai_response['Fraud Likelihood'])

    df.at[row_index, 'fraud_percent'] = fraud_percentage
    df.at[row_index, 'summary'] = processed_openai_response['Summary']

    if processed_openai_response['Summary'] != "No summary available":
        df.at[row_index, 'status'] = "potential_fraud"
    else:
        df.at[row_index, 'status'] = "potential_not_fraud"

    return df

def update_3_rows(filtered_df, processed_openai_response):
    for idx, report in enumerate(processed_openai_response):
        fraud_percentage = re.sub(r'[^0-9]', '', report['Fraud Possibility'])

        df.at[filtered_df.index[idx], 'fraud_percent'] = fraud_percentage
        df.at[filtered_df.index[idx], 'summary'] = report['Summary']

        if report['Summary'] != "No summary available":
            df.at[filtered_df.index[idx], 'status'] = "potential_fraud"
        else:
            df.at[filtered_df.index[idx], 'status'] = "potential_not_fraud"

    return df

# Test for 1 line
def run_single_row(df, row_index):
  if filter_single_status(df.iloc[row_index]):
      message = combine_single_text(df.loc[row_index])
      if message is not None:
          system_message = create_single_system_message()
          messages = [ system_message, {"role": "user", "content": message}]
          raw_openai_response = fetch_single_openai_response(messages)
          processed_openai_response = process_single_response(raw_openai_response)
          update_single_row(df, row_index, processed_openai_response)
          return processed_openai_response
      else:
          return "No message"
  else:
      return "Not Validate"

one_row_result = run_single_row(df, 1)
print(one_row_result)

df

# For 3 line
def run_3_row(df):
  filtered_df = filter_whole_status(df)
  if len(filtered_df) >= 3:
      message = combine_3_text(filtered_df)
      if message is not None:
          system_message = create_3_system_message()
          messages = [ system_message, {"role": "user", "content": message}]
          raw_openai_response = fetch_single_openai_response(messages)
          processed_openai_response = process_3_response(raw_openai_response)
          update_3_rows(filtered_df, processed_openai_response)
          return processed_openai_response
      else:
          return "No message"
  else:
      return "Not Validate"

print(run_3_row(df.loc[0:2]))
three_row_result = run_3_row(df.loc[0:3])
print(three_row_result)

# To summarise those comfirmed reports

from transformers import pipeline

summarizer = pipeline("summarization")

scam_sentences = [
    "Investment scam offering to double money in 30 days with a $1,000 upfront payment and a suspicious website.",
    "Investment scam asking for RM1,000 upfront to double money in 30 days.",
    "Investment scam promising high returns in 30 days with an upfront payment of RM1,000 and a questionable website link."
]

combined_text = " ".join(scam_sentences)
summary = summarizer(combined_text, max_length=50, min_length=30, do_sample=False)

print("Summary:", summary[0]['summary_text'])

